{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f274214",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP421 Data Mining - Assignment 2</div>\n",
    "### <div align=\"center\">Due Date: Nov 6, 2023 at 11:59 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ca562",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2e9e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90631e8",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff4dec",
   "metadata": {},
   "source": [
    "#### 1. Tokenization \n",
    "Tokenization refers to the process of breaking down a piece of text into smaller units, commonly known as tokens. Typically, tokens are words, but they can also be phrases, sentences, or any other unit that makes sense for the specific analysis. Tokenization is essential because it helps convert the unstructured form of textual data into a form that can be utilized in various Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43eec7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, on, Greenspan, speech, The, do...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, The, ...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, to, beat, dialler, scams, BT, is...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [Be, careful, how, you, code, A, new, European...\n",
      "2223    [US, cyber, security, chief, resigns, The, man...\n",
      "2224    [Losing, yourself, in, online, gaming, Online,...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filename = 'bbc-news-data-modified.csv'\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "df['Tokens'] = df['title'] + ' ' + df['content']\n",
    "\n",
    "tokens = df['Tokens'].str.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8631a65",
   "metadata": {},
   "source": [
    "#### 2. Stop Words Removal \n",
    "Stop words removal are commonly used words in any language which don’t add much meaning to a sentence. Words like ‘and’, ‘the’, ‘is’, and ‘in’ are examples of stop words. In text mining and search engines, these words are eliminated from the text to expedite the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d76f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, Greenspan, speech, dollar, hit...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, owner...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, beat, dialler, scams, BT, introd...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [careful, code, new, European, directive, coul...\n",
      "2223    [US, cyber, security, chief, resigns, man, mak...\n",
      "2224    [Losing, online, gaming, Online, role, playing...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = tokens.apply(lambda words: [word for word in words if word.lower() not in stop_words])\n",
    "# Now, df contains the 'Words' column with stop words removed\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df0540",
   "metadata": {},
   "source": [
    "#### 3. Lemmatization and Stemming \n",
    "Lemmatization and stemming involves converting a word to its base or dictionary form. For instance, ‘running’ becomes ‘run’, ‘better’ becomes ‘good’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048900d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [ad, sale, boost, time, warner, profit, quarte...\n",
      "1       [dollar, gain, greenspan, speech, dollar, hit,...\n",
      "2       [yuko, unit, buyer, face, loan, claim, owner, ...\n",
      "3       [high, fuel, price, hit, ba', profit, british,...\n",
      "4       [pernod, takeov, talk, lift, domecq, share, uk...\n",
      "                              ...                        \n",
      "2220    [bt, program, beat, dialler, scam, bt, introdu...\n",
      "2221    [spam, e-mail, tempt, net, shopper, comput, us...\n",
      "2222    [care, code, new, european, direct, could, put...\n",
      "2223    [us, cyber, secur, chief, resign, man, make, s...\n",
      "2224    [lose, onlin, game, onlin, role, play, game, t...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = tokens.apply(lambda words: [stemmer.stem(lemmatizer.lemmatize(word)) for word in words])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eaf450",
   "metadata": {},
   "source": [
    "#### 4. Vectorizerization\n",
    "Vectorizerization can be used to transform the text data into a matrix of TF-IDF features. This matrix serves as the input for the clustering algorithms, enabling them to cluster the documents based on the significance and occurrence of terms within them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7607ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 27259)\n",
      "    00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0  0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3  0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "   zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "   zvyagintsev  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "\n",
      "[5 rows x 27259 columns]\n"
     ]
    }
   ],
   "source": [
    "documents = [' '.join(doc) for doc in tokens]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the 'Words' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "text = pd.DataFrame(data=tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now, print your DataFrame\n",
    "print(text.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1989f",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6180d",
   "metadata": {},
   "source": [
    "#### 1. K-means Clustering: \n",
    "With the processed text, segregate the news articles into 5 distinct clusters using the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f97c438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0     0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3     0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "...   ...       ...   ...    ...   ...    ...  ...     ...    ...  ...  ...   \n",
      "2220  0.0  0.046149   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2221  0.0  0.026533   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2222  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2223  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2224  0.0  0.006765   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "      zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "...       ...       ...    ...     ...      ...     ...    ...        ...   \n",
      "2220      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2221      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2222      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2223      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2224      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "      zvyagintsev  cluster  \n",
      "0             0.0        4  \n",
      "1             0.0        4  \n",
      "2             0.0        4  \n",
      "3             0.0        4  \n",
      "4             0.0        4  \n",
      "...           ...      ...  \n",
      "2220          0.0        1  \n",
      "2221          0.0        1  \n",
      "2222          0.0        4  \n",
      "2223          0.0        1  \n",
      "2224          0.0        3  \n",
      "\n",
      "[2225 rows x 27260 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=5, n_init=10, random_state=1)\n",
    "\n",
    "#fit k-means algorithm to data\n",
    "kmeans.fit(text)\n",
    "kmeans.labels_\n",
    "\n",
    "text['cluster'] = kmeans.labels_\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3e122f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    677\n",
      "1    548\n",
      "4    353\n",
      "3    348\n",
      "0    299\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_values_counts = text['cluster'].value_counts()\n",
    "# Print the result\n",
    "print(unique_values_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092b218",
   "metadata": {},
   "source": [
    "#### 2. DBSCAN Clustering: \n",
    "Utilize the preprocessed text and implement the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "735d5a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "dbs = DBSCAN(eps=0.03, min_samples=2).fit(text)\n",
    "labels = dbs.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92ecee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 83\n",
      "Estimated number of noise points: 2059\n"
     ]
    }
   ],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24318b",
   "metadata": {},
   "source": [
    "#### 3. Gaussian Mixture Model (GMM) Clustering: \n",
    "Allocate the news articles into 5 clusters leveraging the Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9647c5e2",
   "metadata": {},
   "source": [
    "#### 4. Agglomerative Clustering: \n",
    "Administer Agglomerative Clustering on the processed text, aiming to form 5 Coherent Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03981f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
