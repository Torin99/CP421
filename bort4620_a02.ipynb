{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea69e1d8",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP421 Data Mining - Assignment 2</div>\n",
    "### <div align=\"center\">Due Date: Nov 6, 2023 at 11:59 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f93e8",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e9e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90631e8",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b4e68",
   "metadata": {},
   "source": [
    "#### 1. Tokenization \n",
    "Tokenization refers to the process of breaking down a piece of text into smaller units, commonly known as tokens. Typically, tokens are words, but they can also be phrases, sentences, or any other unit that makes sense for the specific analysis. Tokenization is essential because it helps convert the unstructured form of textual data into a form that can be utilized in various Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425791b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, on, Greenspan, speech, The, do...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, The, ...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, to, beat, dialler, scams, BT, is...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [Be, careful, how, you, code, A, new, European...\n",
      "2223    [US, cyber, security, chief, resigns, The, man...\n",
      "2224    [Losing, yourself, in, online, gaming, Online,...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filename = 'bbc-news-data-modified.csv'\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "df['Tokens'] = df['title'] + ' ' + df['content']\n",
    "\n",
    "tokens = df['Tokens'].str.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca2f72",
   "metadata": {},
   "source": [
    "#### 2. Stop Words Removal \n",
    "Stop words removal are commonly used words in any language which don’t add much meaning to a sentence. Words like ‘and’, ‘the’, ‘is’, and ‘in’ are examples of stop words. In text mining and search engines, these words are eliminated from the text to expedite the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d38f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, Greenspan, speech, dollar, hit...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, owner...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, beat, dialler, scams, BT, introd...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [careful, code, new, European, directive, coul...\n",
      "2223    [US, cyber, security, chief, resigns, man, mak...\n",
      "2224    [Losing, online, gaming, Online, role, playing...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = tokens.apply(lambda words: [word for word in words if word.lower() not in stop_words])\n",
    "# Now, df contains the 'Words' column with stop words removed\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5715a9",
   "metadata": {},
   "source": [
    "#### 3. Lemmatization and Stemming \n",
    "Lemmatization and stemming involves converting a word to its base or dictionary form. For instance, ‘running’ becomes ‘run’, ‘better’ becomes ‘good’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fbabd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [ad, sale, boost, time, warner, profit, quarte...\n",
      "1       [dollar, gain, greenspan, speech, dollar, hit,...\n",
      "2       [yuko, unit, buyer, face, loan, claim, owner, ...\n",
      "3       [high, fuel, price, hit, ba', profit, british,...\n",
      "4       [pernod, takeov, talk, lift, domecq, share, uk...\n",
      "                              ...                        \n",
      "2220    [bt, program, beat, dialler, scam, bt, introdu...\n",
      "2221    [spam, e-mail, tempt, net, shopper, comput, us...\n",
      "2222    [care, code, new, european, direct, could, put...\n",
      "2223    [us, cyber, secur, chief, resign, man, make, s...\n",
      "2224    [lose, onlin, game, onlin, role, play, game, t...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = tokens.apply(lambda words: [stemmer.stem(lemmatizer.lemmatize(word)) for word in words])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d505bf",
   "metadata": {},
   "source": [
    "#### 4. Vectorizerization\n",
    "Vectorizerization can be used to transform the text data into a matrix of TF-IDF features. This matrix serves as the input for the clustering algorithms, enabling them to cluster the documents based on the significance and occurrence of terms within them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c29e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0  0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3  0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "   zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "   zvyagintsev  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "\n",
      "[5 rows x 27259 columns]\n"
     ]
    }
   ],
   "source": [
    "documents = [' '.join(doc) for doc in tokens]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the 'Words' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "text = pd.DataFrame(data=tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now, print your DataFrame\n",
    "print(text.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b3183",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f2fde",
   "metadata": {},
   "source": [
    "#### 1. K-means Clustering: \n",
    "With the processed text, segregate the news articles into 5 distinct clusters using the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d094081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=5, n_init=10, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(init=&#x27;random&#x27;, n_clusters=5, n_init=10, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(init='random', n_clusters=5, n_init=10, random_state=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=5, n_init=10, random_state=1)\n",
    "\n",
    "#fit k-means algorithm to data\n",
    "kmeans.fit(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef300c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0     0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3     0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "...   ...       ...   ...    ...   ...    ...  ...     ...    ...  ...  ...   \n",
      "2220  0.0  0.046149   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2221  0.0  0.026533   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2222  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2223  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2224  0.0  0.006765   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "      zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "...       ...       ...    ...     ...      ...     ...    ...        ...   \n",
      "2220      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2221      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2222      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2223      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2224      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "      zvyagintsev  cluster  \n",
      "0             0.0        2  \n",
      "1             0.0        2  \n",
      "2             0.0        2  \n",
      "3             0.0        2  \n",
      "4             0.0        2  \n",
      "...           ...      ...  \n",
      "2220          0.0        4  \n",
      "2221          0.0        4  \n",
      "2222          0.0        2  \n",
      "2223          0.0        4  \n",
      "2224          0.0        1  \n",
      "\n",
      "[2225 rows x 27260 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans.labels_\n",
    "\n",
    "text['cluster'] = kmeans.labels_\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6fdddc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    677\n",
      "1    548\n",
      "4    353\n",
      "3    348\n",
      "0    299\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_values_counts = text['cluster'].value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(unique_values_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a7872",
   "metadata": {},
   "source": [
    "#### 2. DBSCAN Clustering: \n",
    "Utilize the preprocessed text and implement the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbe73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acf96329",
   "metadata": {},
   "source": [
    "#### 3. Gaussian Mixture Model (GMM) Clustering: \n",
    "Allocate the news articles into 5 clusters leveraging the Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366f093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e91d048",
   "metadata": {},
   "source": [
    "#### 4. Agglomerative Clustering: \n",
    "Administer Agglomerative Clustering on the processed text, aiming to form 5 Coherent Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495cae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
