{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b133526",
   "metadata": {},
   "source": [
    "# <div align=\"center\">CP421 Data Mining - Assignment 2</div>\n",
    "### <div align=\"center\">Due Date: Nov 6, 2023 at 11:59 PM</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47a387",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d2e9e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\twbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90631e8",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce2d7d",
   "metadata": {},
   "source": [
    "#### 1. Tokenization \n",
    "Tokenization refers to the process of breaking down a piece of text into smaller units, commonly known as tokens. Typically, tokens are words, but they can also be phrases, sentences, or any other unit that makes sense for the specific analysis. Tokenization is essential because it helps convert the unstructured form of textual data into a form that can be utilized in various Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95cee83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, on, Greenspan, speech, The, do...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, The, ...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, to, beat, dialler, scams, BT, is...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [Be, careful, how, you, code, A, new, European...\n",
      "2223    [US, cyber, security, chief, resigns, The, man...\n",
      "2224    [Losing, yourself, in, online, gaming, Online,...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filename = 'bbc-news-data-modified.csv'\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "df['Tokens'] = df['title'] + ' ' + df['content']\n",
    "\n",
    "tokens = df['Tokens'].str.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6485f78",
   "metadata": {},
   "source": [
    "#### 2. Stop Words Removal \n",
    "Stop words removal are commonly used words in any language which don’t add much meaning to a sentence. Words like ‘and’, ‘the’, ‘is’, and ‘in’ are examples of stop words. In text mining and search engines, these words are eliminated from the text to expedite the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "542da36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Ad, sales, boost, Time, Warner, profit, Quart...\n",
      "1       [Dollar, gains, Greenspan, speech, dollar, hit...\n",
      "2       [Yukos, unit, buyer, faces, loan, claim, owner...\n",
      "3       [High, fuel, prices, hit, BA's, profits, Briti...\n",
      "4       [Pernod, takeover, talk, lifts, Domecq, Shares...\n",
      "                              ...                        \n",
      "2220    [BT, program, beat, dialler, scams, BT, introd...\n",
      "2221    [Spam, e-mails, tempt, net, shoppers, Computer...\n",
      "2222    [careful, code, new, European, directive, coul...\n",
      "2223    [US, cyber, security, chief, resigns, man, mak...\n",
      "2224    [Losing, online, gaming, Online, role, playing...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = tokens.apply(lambda words: [word for word in words if word.lower() not in stop_words])\n",
    "# Now, df contains the 'Words' column with stop words removed\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcd6e0",
   "metadata": {},
   "source": [
    "#### 3. Lemmatization and Stemming \n",
    "Lemmatization and stemming involves converting a word to its base or dictionary form. For instance, ‘running’ becomes ‘run’, ‘better’ becomes ‘good’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "896e1d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [ad, sale, boost, time, warner, profit, quarte...\n",
      "1       [dollar, gain, greenspan, speech, dollar, hit,...\n",
      "2       [yuko, unit, buyer, face, loan, claim, owner, ...\n",
      "3       [high, fuel, price, hit, ba', profit, british,...\n",
      "4       [pernod, takeov, talk, lift, domecq, share, uk...\n",
      "                              ...                        \n",
      "2220    [bt, program, beat, dialler, scam, bt, introdu...\n",
      "2221    [spam, e-mail, tempt, net, shopper, comput, us...\n",
      "2222    [care, code, new, european, direct, could, put...\n",
      "2223    [us, cyber, secur, chief, resign, man, make, s...\n",
      "2224    [lose, onlin, game, onlin, role, play, game, t...\n",
      "Name: Tokens, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = tokens.apply(lambda words: [stemmer.stem(lemmatizer.lemmatize(word)) for word in words])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a50a66",
   "metadata": {},
   "source": [
    "#### 4. Vectorizerization\n",
    "Vectorizerization can be used to transform the text data into a matrix of TF-IDF features. This matrix serves as the input for the clustering algorithms, enabling them to cluster the documents based on the significance and occurrence of terms within them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d56a5c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 27259)\n",
      "    00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0  0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3  0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "   zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4    0.0      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "   zvyagintsev  \n",
      "0          0.0  \n",
      "1          0.0  \n",
      "2          0.0  \n",
      "3          0.0  \n",
      "4          0.0  \n",
      "\n",
      "[5 rows x 27259 columns]\n"
     ]
    }
   ],
   "source": [
    "documents = [' '.join(doc) for doc in tokens]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the 'Words' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "text = pd.DataFrame(data=tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Now, print your DataFrame\n",
    "print(text.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aef70c",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2548be",
   "metadata": {},
   "source": [
    "#### 1. K-means Clustering: \n",
    "With the processed text, segregate the news articles into 5 distinct clusters using the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "787c223f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00       000  0001  000bn  000m  000th  001  001and  001st  004  ...  \\\n",
      "0     0.0  0.022250   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "1     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "3     0.0  0.020255   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "4     0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "...   ...       ...   ...    ...   ...    ...  ...     ...    ...  ...  ...   \n",
      "2220  0.0  0.046149   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2221  0.0  0.026533   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2222  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2223  0.0  0.000000   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "2224  0.0  0.006765   0.0    0.0   0.0    0.0  0.0     0.0    0.0  0.0  ...   \n",
      "\n",
      "      zooropa  zornotza  zorro  zubair  zuluaga  zurich  zuton  zvonareva  \\\n",
      "0         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "1         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "3         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "4         0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "...       ...       ...    ...     ...      ...     ...    ...        ...   \n",
      "2220      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2221      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2222      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2223      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "2224      0.0       0.0    0.0     0.0      0.0     0.0    0.0        0.0   \n",
      "\n",
      "      zvyagintsev  cluster  \n",
      "0             0.0        2  \n",
      "1             0.0        2  \n",
      "2             0.0        2  \n",
      "3             0.0        2  \n",
      "4             0.0        2  \n",
      "...           ...      ...  \n",
      "2220          0.0        4  \n",
      "2221          0.0        4  \n",
      "2222          0.0        2  \n",
      "2223          0.0        4  \n",
      "2224          0.0        1  \n",
      "\n",
      "[2225 rows x 27260 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(init=\"random\", n_clusters=5, n_init=10, random_state=1)\n",
    "\n",
    "#fit k-means algorithm to data\n",
    "kmeans.fit(text)\n",
    "kmeans.labels_\n",
    "\n",
    "text['cluster'] = kmeans.labels_\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53a7c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    787\n",
      "1    517\n",
      "3    351\n",
      "4    343\n",
      "0    227\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_values_counts = text['cluster'].value_counts()\n",
    "# Print the result\n",
    "print(unique_values_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9420939",
   "metadata": {},
   "source": [
    "#### 2. DBSCAN Clustering: \n",
    "Utilize the preprocessed text and implement the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "209f3751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "dbs = DBSCAN(eps=0.03, min_samples=2).fit(text)\n",
    "labels = dbs.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66bb0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 112\n",
      "Estimated number of noise points: 2001\n"
     ]
    }
   ],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6710df8",
   "metadata": {},
   "source": [
    "#### 3. Gaussian Mixture Model (GMM) Clustering: \n",
    "Allocate the news articles into 5 clusters leveraging the Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9359c5bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.54 GiB for an array with shape (27260, 27260) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming all columns are your features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Fit the GMM to the feature matrix\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m gmm\u001b[38;5;241m.\u001b[39mfit(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_predict(X, y)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_base.py:235\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_verbose_msg_init_beg(init)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_init:\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_parameters(X, random_state)\n\u001b[0;32m    237\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mif\u001b[39;00m do_init \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower_bound_\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_base.py:140\u001b[0m, in \u001b[0;36mBaseMixture._initialize_parameters\u001b[1;34m(self, X, random_state)\u001b[0m\n\u001b[0;32m    133\u001b[0m     _, indices \u001b[38;5;241m=\u001b[39m kmeans_plusplus(\n\u001b[0;32m    134\u001b[0m         X,\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components,\n\u001b[0;32m    136\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m     resp[indices, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(X, resp)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:713\u001b[0m, in \u001b[0;36mGaussianMixture._initialize\u001b[1;34m(self, X, resp)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialization of the Gaussian mixture parameters.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03mresp : array-like of shape (n_samples, n_components)\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m n_samples, _ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 713\u001b[0m weights, means, covariances \u001b[38;5;241m=\u001b[39m _estimate_gaussian_parameters(\n\u001b[0;32m    714\u001b[0m     X, resp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    715\u001b[0m )\n\u001b[0;32m    716\u001b[0m weights \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_init\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:290\u001b[0m, in \u001b[0;36m_estimate_gaussian_parameters\u001b[1;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[0;32m    288\u001b[0m nk \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(resp\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m    289\u001b[0m means \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(resp\u001b[38;5;241m.\u001b[39mT, X) \u001b[38;5;241m/\u001b[39m nk[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m--> 290\u001b[0m covariances \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_full,\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_tied,\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_diag,\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspherical\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_spherical,\n\u001b[0;32m    295\u001b[0m }[covariance_type](resp, X, nk, means, reg_covar)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nk, means, covariances\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:177\u001b[0m, in \u001b[0;36m_estimate_gaussian_covariances_full\u001b[1;34m(resp, X, nk, means, reg_covar)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components):\n\u001b[0;32m    176\u001b[0m     diff \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m means[k]\n\u001b[1;32m--> 177\u001b[0m     covariances[k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(resp[:, k] \u001b[38;5;241m*\u001b[39m diff\u001b[38;5;241m.\u001b[39mT, diff) \u001b[38;5;241m/\u001b[39m nk[k]\n\u001b[0;32m    178\u001b[0m     covariances[k]\u001b[38;5;241m.\u001b[39mflat[:: n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reg_covar\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m covariances\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.54 GiB for an array with shape (27260, 27260) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "# Initialize and fit a Gaussian Mixture Model\n",
    "n_components = 1  # Set the number of components (clusters) you want\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "\n",
    "# Extract the feature matrix from the DataFrame\n",
    "X = text.values  # Assuming all columns are your features\n",
    "\n",
    "# Fit the GMM to the feature matrix\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bd6e8",
   "metadata": {},
   "source": [
    "#### 4. Agglomerative Clustering: \n",
    "Administer Agglomerative Clustering on the processed text, aiming to form 5 Coherent Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "18f2b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_clustering = AgglomerativeClustering(n_clusters = 5).fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cfa7df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 2]\n",
      "5\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(clustering.labels_)\n",
    "n_clus_ = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n",
    "n_noise_ = list(clustering.labels_).count(-1)\n",
    "\n",
    "print(n_clus_)\n",
    "print(n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0f14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
